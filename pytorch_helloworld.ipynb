{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "163b26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9eb1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('./kaggle-mnist/Data/train.csv')\n",
    "#feature\n",
    "feature = raw_df.drop(['label'], axis=1).values\n",
    "#https://medium.com/hccuse/pandas%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-%E8%BB%B8%E5%90%91%E7%90%86%E8%A7%A3-1084c6c5e873\n",
    "\n",
    "#label\n",
    "label = raw_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7e4c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 42000\n"
     ]
    }
   ],
   "source": [
    "print(len(label), len(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b142d720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600 33600 8400 8400\n"
     ]
    }
   ],
   "source": [
    "train_feature = feature[:int(len(feature)*0.8)]\n",
    "train_label = label[:int(len(label)*0.8)]\n",
    "\n",
    "test_feature = feature[:int(len(feature)*0.2)]\n",
    "test_label = label[:int(len(label)*0.2)]\n",
    "print(len(train_feature), len(train_label), len(test_feature), len(test_label))\n",
    "\n",
    "train_feature = torch.tensor(train_feature).to(torch.float).cuda()\n",
    "train_label = torch.tensor(train_label).cuda()\n",
    "test_feature = torch.tensor(test_feature).to(torch.float).cuda()\n",
    "test_label = torch.tensor(test_label).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01ae3286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=444, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=444, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 444),#wx+b\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(444, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10),\n",
    "#     nn.Softmax()#CrossEntropyLoss時會默認算一次softmax\n",
    ").cuda()#也可以放在這\n",
    "\n",
    "#將資料放入gpu ram\n",
    "# model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dad3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.rand(1,784)\n",
    "# pred = model(data)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90d261fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfunction = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e20b3ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:5.744922161102295 train acc:0.11937499791383743\n",
      "test loss:3.9235002994537354 test acc:0.1522618979215622\n",
      "train loss:3.883194923400879 train acc:0.15160714089870453\n",
      "test loss:3.0466742515563965 test acc:0.25999999046325684\n",
      "train loss:3.039297580718994 train acc:0.2579761743545532\n",
      "test loss:2.6159260272979736 test acc:0.3372619152069092\n",
      "train loss:2.6265716552734375 train acc:0.3311011791229248\n",
      "test loss:2.310678243637085 test acc:0.3963095247745514\n",
      "train loss:2.3321468830108643 train acc:0.39175593852996826\n",
      "test loss:1.9959758520126343 test acc:0.46857142448425293\n",
      "train loss:2.0252480506896973 train acc:0.4642857015132904\n",
      "test loss:1.7283117771148682 test acc:0.5248809456825256\n",
      "train loss:1.7613660097122192 train acc:0.5205357074737549\n",
      "test loss:1.4713302850723267 test acc:0.5682142972946167\n",
      "train loss:1.5019633769989014 train acc:0.5647023916244507\n",
      "test loss:1.2066363096237183 test acc:0.6279761791229248\n",
      "train loss:1.2322171926498413 train acc:0.6230059266090393\n",
      "test loss:0.9952253103256226 test acc:0.6783333420753479\n",
      "train loss:1.0155949592590332 train acc:0.6777083277702332\n",
      "test loss:0.8654928803443909 test acc:0.7216666340827942\n",
      "train loss:0.8818525671958923 train acc:0.7183631062507629\n",
      "test loss:0.8053015470504761 test acc:0.7386904954910278\n",
      "train loss:0.8185572028160095 train acc:0.736339271068573\n",
      "test loss:0.7911368012428284 test acc:0.7465476393699646\n",
      "train loss:0.8021942377090454 train acc:0.7418452501296997\n",
      "test loss:0.7916409969329834 test acc:0.746904730796814\n",
      "train loss:0.8016077280044556 train acc:0.741398811340332\n",
      "test loss:0.7768684029579163 test acc:0.7545238137245178\n",
      "train loss:0.7862021923065186 train acc:0.7478868961334229\n",
      "test loss:0.7342331409454346 test acc:0.7698809504508972\n",
      "train loss:0.7430176734924316 train acc:0.7627678513526917\n",
      "test loss:0.6717795729637146 test acc:0.7878571152687073\n",
      "train loss:0.6800507307052612 train acc:0.7825595140457153\n",
      "test loss:0.6063196063041687 test acc:0.807619035243988\n",
      "train loss:0.6142472624778748 train acc:0.8046725988388062\n",
      "test loss:0.5511546730995178 test acc:0.8235714435577393\n",
      "train loss:0.5590690970420837 train acc:0.8222321271896362\n",
      "test loss:0.512039840221405 test acc:0.8384523987770081\n",
      "train loss:0.5202541947364807 train acc:0.834940493106842\n",
      "test loss:0.48839321732521057 test acc:0.8478571176528931\n",
      "train loss:0.497073233127594 train acc:0.8431845307350159\n",
      "test loss:0.4759015738964081 test acc:0.8540475964546204\n",
      "train loss:0.4851858913898468 train acc:0.8480059504508972\n",
      "test loss:0.469146728515625 test acc:0.8550000190734863\n",
      "train loss:0.47907063364982605 train acc:0.8494940400123596\n",
      "test loss:0.4633793830871582 test acc:0.8571428656578064\n",
      "train loss:0.4737810790538788 train acc:0.8516071438789368\n",
      "test loss:0.4553292691707611 test acc:0.8596428632736206\n",
      "train loss:0.4659580588340759 train acc:0.8535416722297668\n",
      "test loss:0.44371867179870605 test acc:0.8638095259666443\n",
      "train loss:0.4542725682258606 train acc:0.8578869104385376\n",
      "test loss:0.42896556854248047 test acc:0.8702380657196045\n",
      "train loss:0.43909820914268494 train acc:0.8636606931686401\n",
      "test loss:0.4124346673488617 test acc:0.8741666674613953\n",
      "train loss:0.42184507846832275 train acc:0.8691071271896362\n",
      "test loss:0.39556238055229187 test acc:0.878928542137146\n",
      "train loss:0.40410321950912476 train acc:0.8746428489685059\n",
      "test loss:0.37952256202697754 test acc:0.8846428394317627\n",
      "train loss:0.3872492015361786 train acc:0.8795535564422607\n",
      "test loss:0.3651960790157318 test acc:0.8899999856948853\n",
      "train loss:0.3722648620605469 train acc:0.8855952024459839\n",
      "test loss:0.3530971109867096 test acc:0.8940476179122925\n",
      "train loss:0.35973861813545227 train acc:0.8899999856948853\n",
      "test loss:0.34349000453948975 test acc:0.8974999785423279\n",
      "train loss:0.3499189019203186 train acc:0.8929761648178101\n",
      "test loss:0.3362639546394348 test acc:0.8986904621124268\n",
      "train loss:0.34264642000198364 train acc:0.8952381014823914\n",
      "test loss:0.3309067487716675 test acc:0.9007142782211304\n",
      "train loss:0.3373400866985321 train acc:0.8971726298332214\n",
      "test loss:0.32658612728118896 test acc:0.9026190638542175\n",
      "train loss:0.3330483138561249 train acc:0.8987797498703003\n",
      "test loss:0.32226258516311646 test acc:0.9038094878196716\n",
      "train loss:0.3287082314491272 train acc:0.8997321128845215\n",
      "test loss:0.3170866072177887 test acc:0.9057142734527588\n",
      "train loss:0.32346272468566895 train acc:0.9017261862754822\n",
      "test loss:0.3106479048728943 test acc:0.9090476036071777\n",
      "train loss:0.31689509749412537 train acc:0.9044047594070435\n",
      "test loss:0.3030465543270111 test acc:0.9115476012229919\n",
      "train loss:0.3091179430484772 train acc:0.9066368937492371\n",
      "test loss:0.2948315143585205 test acc:0.9141666293144226\n",
      "train loss:0.3007342219352722 train acc:0.9094940423965454\n",
      "test loss:0.28671199083328247 test acc:0.916785717010498\n",
      "train loss:0.2924703359603882 train acc:0.9121130704879761\n",
      "test loss:0.2793339192867279 test acc:0.9190475940704346\n",
      "train loss:0.2849375903606415 train acc:0.9143452048301697\n",
      "test loss:0.27303004264831543 test acc:0.9197618961334229\n",
      "train loss:0.27848565578460693 train acc:0.9155059456825256\n",
      "test loss:0.2677816152572632 test acc:0.9209523797035217\n",
      "train loss:0.2730996608734131 train acc:0.9172916412353516\n",
      "test loss:0.26336899399757385 test acc:0.9220237731933594\n",
      "train loss:0.26854994893074036 train acc:0.9188988208770752\n",
      "test loss:0.25946834683418274 test acc:0.9232142567634583\n",
      "train loss:0.26449844241142273 train acc:0.9203869104385376\n",
      "test loss:0.255750447511673 test acc:0.9245237708091736\n",
      "train loss:0.2606044113636017 train acc:0.9217261672019958\n",
      "test loss:0.25199124217033386 test acc:0.9253571033477783\n",
      "train loss:0.2566252648830414 train acc:0.9225297570228577\n",
      "test loss:0.24804362654685974 test acc:0.927142858505249\n",
      "train loss:0.2524341642856598 train acc:0.9238988161087036\n",
      "test loss:0.24386590719223022 test acc:0.9282142519950867\n",
      "train loss:0.24801556766033173 train acc:0.9252380728721619\n",
      "test loss:0.2395525872707367 test acc:0.9292857050895691\n",
      "train loss:0.24345232546329498 train acc:0.9267261624336243\n",
      "test loss:0.23523512482643127 test acc:0.9313095211982727\n",
      "train loss:0.23887936770915985 train acc:0.9280952215194702\n",
      "test loss:0.23103612661361694 test acc:0.9321428537368774\n",
      "train loss:0.234431192278862 train acc:0.9291666746139526\n",
      "test loss:0.22704468667507172 test acc:0.9342857003211975\n",
      "train loss:0.2302040010690689 train acc:0.9305654764175415\n",
      "test loss:0.22329163551330566 test acc:0.9355952143669128\n",
      "train loss:0.22624990344047546 train acc:0.9317857027053833\n",
      "test loss:0.21979786455631256 test acc:0.9372618794441223\n",
      "train loss:0.2225845754146576 train acc:0.9330952167510986\n",
      "test loss:0.2165292501449585 test acc:0.9384523630142212\n",
      "train loss:0.21918001770973206 train acc:0.9346428513526917\n",
      "test loss:0.2134132832288742 test acc:0.9396428465843201\n",
      "train loss:0.21597172319889069 train acc:0.9355952143669128\n",
      "test loss:0.2103821039199829 test acc:0.9402380585670471\n",
      "train loss:0.21285949647426605 train acc:0.9365773797035217\n",
      "test loss:0.20731669664382935 test acc:0.9411904811859131\n",
      "train loss:0.20972894132137299 train acc:0.9374702572822571\n",
      "test loss:0.20414841175079346 test acc:0.9417856931686401\n",
      "train loss:0.2065018266439438 train acc:0.9383035898208618\n",
      "test loss:0.20090018212795258 test acc:0.9427380561828613\n",
      "train loss:0.2031809240579605 train acc:0.9395833015441895\n",
      "test loss:0.19763724505901337 test acc:0.9436904788017273\n",
      "train loss:0.19983842968940735 train acc:0.9408630728721619\n",
      "test loss:0.19444981217384338 test acc:0.9446428418159485\n",
      "train loss:0.19658029079437256 train acc:0.9422321319580078\n",
      "test loss:0.1914128214120865 test acc:0.9454761743545532\n",
      "train loss:0.19346490502357483 train acc:0.9436011910438538\n",
      "test loss:0.18853771686553955 test acc:0.9461904764175415\n",
      "train loss:0.19049501419067383 train acc:0.9443749785423279\n",
      "test loss:0.18578574061393738 test acc:0.9472618699073792\n",
      "train loss:0.18764109909534454 train acc:0.9449999928474426\n",
      "test loss:0.18312256038188934 test acc:0.9471428394317627\n",
      "train loss:0.18486249446868896 train acc:0.9455059170722961\n",
      "test loss:0.18052279949188232 test acc:0.9476190209388733\n",
      "train loss:0.18213531374931335 train acc:0.9463987946510315\n",
      "test loss:0.17796453833580017 test acc:0.9490476250648499\n",
      "train loss:0.17944154143333435 train acc:0.9475595355033875\n",
      "test loss:0.17544353008270264 test acc:0.949404776096344\n",
      "train loss:0.17678099870681763 train acc:0.948273777961731\n",
      "test loss:0.1729259341955185 test acc:0.949999988079071\n",
      "train loss:0.17413979768753052 train acc:0.9490178227424622\n",
      "test loss:0.17039082944393158 test acc:0.9508333206176758\n",
      "train loss:0.17150582373142242 train acc:0.949821412563324\n",
      "test loss:0.1678568571805954 test acc:0.9513095021247864\n",
      "train loss:0.16889409720897675 train acc:0.9502678513526917\n",
      "test loss:0.1653437316417694 test acc:0.9516666531562805\n",
      "train loss:0.16633003950119019 train acc:0.9510118961334229\n",
      "test loss:0.1628885716199875 test acc:0.9522618651390076\n",
      "train loss:0.1638452410697937 train acc:0.951755940914154\n",
      "test loss:0.1605082005262375 test acc:0.9535714387893677\n",
      "train loss:0.16144035756587982 train acc:0.9527975916862488\n",
      "test loss:0.15818865597248077 test acc:0.9541666507720947\n",
      "train loss:0.15909859538078308 train acc:0.953720211982727\n",
      "test loss:0.15589995682239532 test acc:0.9545238018035889\n",
      "train loss:0.15679021179676056 train acc:0.9542856812477112\n",
      "test loss:0.1536318063735962 test acc:0.9553571343421936\n",
      "train loss:0.15450125932693481 train acc:0.9552083015441895\n",
      "test loss:0.15139810740947723 test acc:0.9559523463249207\n",
      "train loss:0.15223313868045807 train acc:0.9560118913650513\n",
      "test loss:0.14919604361057281 test acc:0.9570237994194031\n",
      "train loss:0.14998960494995117 train acc:0.9564880728721619\n",
      "test loss:0.14703762531280518 test acc:0.9579761624336243\n",
      "train loss:0.14777891337871552 train acc:0.9571726322174072\n",
      "test loss:0.1449231654405594 test acc:0.9589285850524902\n",
      "train loss:0.14559872448444366 train acc:0.9582440257072449\n",
      "test loss:0.14285308122634888 test acc:0.9591666460037231\n",
      "train loss:0.1434505134820938 train acc:0.9588987827301025\n",
      "test loss:0.14082220196723938 test acc:0.9599999785423279\n",
      "train loss:0.1413436084985733 train acc:0.9595833420753479\n",
      "test loss:0.13882912695407867 test acc:0.9609523415565491\n",
      "train loss:0.13928057253360748 train acc:0.9599999785423279\n",
      "test loss:0.1368846893310547 test acc:0.961904764175415\n",
      "train loss:0.13726264238357544 train acc:0.9606547355651855\n",
      "test loss:0.13496631383895874 test acc:0.9623809456825256\n",
      "train loss:0.13527658581733704 train acc:0.9612500071525574\n",
      "test loss:0.13306517899036407 test acc:0.9627380967140198\n",
      "train loss:0.13331907987594604 train acc:0.9619940519332886\n",
      "test loss:0.13118869066238403 test acc:0.9639285802841187\n",
      "train loss:0.13138553500175476 train acc:0.9629464149475098\n",
      "test loss:0.12932147085666656 test acc:0.9651190638542175\n",
      "train loss:0.1294802725315094 train acc:0.9637797474861145\n",
      "test loss:0.1274804323911667 test acc:0.9657142758369446\n",
      "train loss:0.12761008739471436 train acc:0.9645535349845886\n",
      "test loss:0.12566278874874115 test acc:0.9669047594070435\n",
      "train loss:0.1257738471031189 train acc:0.9652678370475769\n",
      "test loss:0.12386947125196457 test acc:0.9674999713897705\n",
      "train loss:0.12396682053804398 train acc:0.9658035635948181\n",
      "test loss:0.12209580093622208 test acc:0.9676190614700317\n",
      "train loss:0.12218771874904633 train acc:0.9662500023841858\n",
      "test loss:0.1203463152050972 test acc:0.9680952429771423\n",
      "train loss:0.12043260037899017 train acc:0.9666071534156799\n",
      "test loss:0.1186319887638092 test acc:0.9684523940086365\n",
      "train loss:0.11870501935482025 train acc:0.966994047164917\n",
      "test loss:0.11694815009832382 test acc:0.9688094854354858\n",
      "train loss:0.11699870228767395 train acc:0.9674404859542847\n",
      "test loss:0.11529266089200974 test acc:0.9694047570228577\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    #first\n",
    "    optimizer.zero_grad()\n",
    "    predict_train = model(train_feature)\n",
    "    result_train = torch.argmax(predict_train, axis=1)\n",
    "    train_acc = torch.mean((result_train == train_label).to(torch.float))\n",
    "#     print(train_acc)\n",
    "    loss_train = lossfunction(predict_train, train_label)\n",
    "    loss_train.backward()#先傳播再梯度下降\n",
    "    optimizer.step()\n",
    "#     print(loss.item())\n",
    "\n",
    "    print(f'train loss:{loss_train.item()} train acc:{train_acc}')\n",
    "    \n",
    "    #second on test\n",
    "    optimizer.zero_grad()\n",
    "    predict_test = model(test_feature)\n",
    "    result_test = torch.argmax(predict_test, axis=1)\n",
    "    test_acc = torch.mean((result_test == test_label).to(torch.float))\n",
    "    loss_test = lossfunction(predict_test, test_label)\n",
    "    print(f'test loss:{loss_test.item()} test acc:{test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d78f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pt')#w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab046d9",
   "metadata": {},
   "source": [
    "# 載入模型參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85f90c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.load('./model.pt')\n",
    "\n",
    "model.load_state_dict(params)#要有之前的model結構，我們只有存參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "614e0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_data = test_feature[100:111]\n",
    "new_test_label = test_label[100:111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fed4dc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "tensor([9, 2, 7, 7, 2, 8, 8, 5, 0, 6, 0])\n",
      "predict\n",
      "tensor([9, 2, 7, 7, 2, 8, 8, 5, 0, 6, 0])\n"
     ]
    }
   ],
   "source": [
    "predict_test = model(new_test_data)\n",
    "result_test = torch.argmax(predict_test, axis=1)\n",
    "print(\"label\")\n",
    "print(new_test_label)\n",
    "\n",
    "print(\"predict\")\n",
    "print(result_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbfcfd5",
   "metadata": {},
   "source": [
    "# 資料與模型可以放在GPU來加速"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
